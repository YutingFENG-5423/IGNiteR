{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from random import randint, sample\n",
    "import os\n",
    "import re\n",
    "from utils import save_pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# title words vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mid_title\n",
    "root_titles = pd.read_csv(\"/media/yuting/TOSHIBA EXT/weibo_new/root_titles_filtered_chinese_only.csv\", \\\n",
    "                          sep='\\t', names=['mid','title_orig','title'], encoding='utf8')\n",
    "# root_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_news(news_dict):\n",
    "    word_dict_raw = {'PADDING': [0, 999999]}\n",
    "    for mid in news_dict:\n",
    "        for word in eval(news_dict[mid]):\n",
    "            if word in word_dict_raw:\n",
    "                word_dict_raw[word][1] += 1\n",
    "            else:\n",
    "                word_dict_raw[word] = [len(word_dict_raw),1]\n",
    "    word_dict = {}\n",
    "    for word in word_dict_raw:\n",
    "        if word_dict_raw[word][1] >= 3:\n",
    "            word_dict[word] = [len(word_dict),word_dict_raw[word][1]]\n",
    "    print(len(word_dict),len(word_dict_raw))\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dict = dict(zip(root_titles.mid,root_titles.title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7275 27231\n"
     ]
    }
   ],
   "source": [
    "word_dict = process_news(news_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load repsot_idlist to map the mid to post_id\n",
    "file_path ='/media/yuting/TOSHIBA EXT/weibo_new/repost_idlist.txt'\n",
    "with open(file_path, \"r\", encoding=\"gbk\") as f:\n",
    "    lines_repost_idlist = f.readlines()\n",
    "# map mid to postid\n",
    "postid_mid_map = {}\n",
    "for i, mid in enumerate(lines_repost_idlist):\n",
    "    postid_mid_map[mid.strip()] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_list = []\n",
    "for k in news_dict:\n",
    "    length = len(eval(news_dict[k]))\n",
    "    length_list.append(length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "postid_mid_map : mid -> post_id (30w)\n",
    "news_index : mid -> 13153\n",
    "news_dict: mid -> title words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mapping word to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title_length = 10\n",
    "def titleword2index(news_dict, word_dict, title_length=10):\n",
    "    news_words = [[0]*title_length]\n",
    "    news_index = {0:0}\n",
    "    for mid in news_dict:\n",
    "        word_id = []\n",
    "        news_index[mid] = len(news_index)\n",
    "        for word in eval(news_dict[mid]):\n",
    "            if word in word_dict:\n",
    "                word_id.append(word_dict[word][0])\n",
    "        word_id = word_id[:title_length]\n",
    "        news_words.append(word_id + [0]*(title_length-len(word_id)))\n",
    "    news_words = np.array(news_words,dtype='int32')\n",
    "    print(len(news_words))\n",
    "    return news_words,news_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13154\n"
     ]
    }
   ],
   "source": [
    "news_words, news_index = titleword2index(news_dict,word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "news_index : mid -> 13153\n",
    "news_words : 13154 x 10 (words matrix for each line of news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get words embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('/media/yuting/TOSHIBA EXT/weibo_words/sgns.weibo.bigram.bz2', \\\n",
    "                                          encoding='utf-8',binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(word_dict,model):\n",
    "    embedding_dict = {}\n",
    "    cnt = 0\n",
    "    for word in word_dict:\n",
    "        try:\n",
    "            embedding_dict[word] = model[word]\n",
    "        except:\n",
    "            pass\n",
    "        if cnt%1000 == 0:\n",
    "            print(cnt,word)\n",
    "        cnt += 1\n",
    "    \n",
    "    embedding_matrix = [0]*len(word_dict)\n",
    "    cand = []\n",
    "    for i in embedding_dict:\n",
    "        embedding_matrix[word_dict[i][0]] = np.array(embedding_dict[i], dtype='float32')\n",
    "        cand.append(embedding_matrix[word_dict[i][0]])\n",
    "    cand = np.array(cand, dtype='float32')\n",
    "    mu = np.mean(cand, axis=0)\n",
    "    Sigma = np.cov(cand.T)\n",
    "    norm = np.random.multivariate_normal(mu, Sigma, 1)\n",
    "    for i in range(len(embedding_matrix)):\n",
    "        if type(embedding_matrix[i]) == int:\n",
    "            embedding_matrix[i] = np.reshape(norm, 300)\n",
    "    embedding_matrix[0] = np.zeros(300, dtype='float32')\n",
    "    embedding_matrix = np.array(embedding_matrix, dtype='float32')\n",
    "    print(embedding_matrix.shape)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 PADDING\n",
      "1000 现身\n",
      "2000 男声\n",
      "3000 文件\n",
      "4000 特殊\n",
      "5000 连锁\n",
      "6000 帮助\n",
      "7000 总是\n",
      "(7275, 300)\n"
     ]
    }
   ],
   "source": [
    "word_embedding_matrix = get_embedding(word_dict,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_embedding : 7275 x 300 (vocab x embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get node embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load target embedding\n",
    "path = '/media/yuting/TOSHIBA EXT/weibo_new/mtl_n_target_embeddings_p.txt'\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "match_number = re.compile('-?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *[-+]?\\ *[0-9]+)?')\n",
    "node2emb = {}\n",
    "emb = ''\n",
    "for i in range(len(lines)):\n",
    "    \n",
    "    if not lines[i].endswith(']]\\n'):\n",
    "        emb = emb + lines[i]\n",
    "        continue\n",
    "    else:\n",
    "        emb = emb + lines[i]\n",
    "        l_temp = re.findall(match_number,emb)\n",
    "        assert len(l_temp)==51\n",
    "        number = [float(x) for x in l_temp[1:]]\n",
    "        node2emb[l_temp[0]] =  number\n",
    "        emb = '' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load source embedding\n",
    "path = '/media/yuting/TOSHIBA EXT/weibo_new/mtl_n_source_embeddings_p.txt'\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines2 = f.readlines()\n",
    "node2emb_source = {}\n",
    "emb_source = ''\n",
    "for i in range(len(lines2)):\n",
    "    if not lines2[i].endswith(']]\\n'):\n",
    "        emb_source = emb_source + lines2[i]\n",
    "        continue\n",
    "    else:\n",
    "        emb_source = emb_source + lines2[i]\n",
    "        l_temp = re.findall(match_number,emb_source)\n",
    "        assert len(l_temp)==51\n",
    "        number = [float(x) for x in l_temp[1:]]\n",
    "        node2emb_source[l_temp[0]] =  number\n",
    "        emb_source = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the source and target embedding\n",
    "node2emb.update(node2emb_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "node2emb: userid -> embedding (source covers target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2emb_new = dict([int(k),v] for k,v in node2emb.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# node embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1787442, 50)\n"
     ]
    }
   ],
   "source": [
    "node_embedding_matrix = [0]*(max(node2emb_new.keys())+2) # to include node null\n",
    "cand = []\n",
    "\n",
    "for i in node2emb_new.keys():\n",
    "    try:\n",
    "        node_embedding_matrix[i+1] = np.array(node2emb_new[i], dtype='float32')\n",
    "    except:\n",
    "        node_embedding_matrix[i+1] = np.zeros(50, dtype='float32')\n",
    "    cand.append(node_embedding_matrix[i+1])\n",
    "cand = np.array(cand, dtype='float32')\n",
    "mu = np.mean(cand, axis=0)\n",
    "Sigma = np.cov(cand.T)\n",
    "norm = np.random.multivariate_normal(mu, Sigma, 1)\n",
    "# print(node_embedding_matrix[0])\n",
    "# print(type(node_embedding_matrix[0]))\n",
    "for i in range(len(node_embedding_matrix)):\n",
    "    if type(node_embedding_matrix[i]) == int:\n",
    "        node_embedding_matrix[i] = np.reshape(norm, 50)\n",
    "# node_embedding_matrix[0] = np.zeros(50, dtype='float32')\n",
    "node_embedding_matrix = np.array(node_embedding_matrix, dtype='float32')\n",
    "print(node_embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reorganize weibo data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "post_reuid_dict : post_id -> (timestamp,userid)\n",
    "postid_mid_map : mid -> post_id (30w)\n",
    "post_id -> mid -> news_index_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path ='/media/yuting/TOSHIBA EXT/weibo_new/repost_data.txt'\n",
    "with open(file_path, \"r\", encoding=\"gbk\") as f:\n",
    "    lines_repost = f.readlines()\n",
    "    \n",
    "postid_reuid_dict = {}\n",
    "i = 0\n",
    "while i < (len(lines_repost)):\n",
    "    A = lines_repost[i].split()[0] # A post_id\n",
    "    B = int(lines_repost[i].split()[1]) # B num of repost\n",
    "    for j in range(B):\n",
    "        postid_reuid_dict.setdefault(A,[]).append(lines_repost[i+1+j].split())\n",
    "    i = i+1+B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000\n",
      "13153\n"
     ]
    }
   ],
   "source": [
    "# filter out useful news\n",
    "mid_history = {}\n",
    "for mid in postid_mid_map.keys():\n",
    "    if int(mid) in news_dict.keys():\n",
    "        mid_history[mid] = postid_reuid_dict[str(postid_mid_map[mid])]\n",
    "print(len(postid_reuid_dict))\n",
    "print(len(mid_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_history = {}\n",
    "for k,v in mid_history.items():\n",
    "#     diffusion_list = mid_history[k]\n",
    "    for i in range(len(v)):\n",
    "        if v[i][1] not in user_history:\n",
    "            user_history.setdefault(v[i][1],[]).append([k,v[i][0]])\n",
    "        else:\n",
    "            user_history[v[i][1]].append([k,v[i][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set time order\n",
    "user_history = dict([k,sorted(v,key=lambda x: x[1])] for k,v in user_history.items())\n",
    "mid_history = dict([k,sorted(v,key=lambda x: x[0])] for k,v in mid_history.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "703"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mid_history['3509462670824667'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# buid up train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select user with threshold1 nb_history \n",
    "def selectUser(user_history_dict, threshold):\n",
    "    uid = []\n",
    "    for k,v in user_history_dict.items():\n",
    "        if len(v) > threshold:\n",
    "            uid.append(k)\n",
    "    print(len(uid))\n",
    "    return uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79109\n"
     ]
    }
   ],
   "source": [
    "uid = selectUser(user_history,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n"
     ]
    }
   ],
   "source": [
    "# split train/test user\n",
    "uid_train = sample(uid,1)\n",
    "uid_test = sample(list(set(uid)-set(uid_train)),1)\n",
    "print(len(uid_train),len(uid_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNodes(user,mid,mid_history,max_nodes_length=30,pos=True):\n",
    "#     print(len(mid_history))\n",
    "    diffusion_chain = mid_history[str(mid)]\n",
    "    if pos:\n",
    "        count = 0\n",
    "        node_list = []\n",
    "        for i in range(len(diffusion_chain)):\n",
    "            if diffusion_chain[i][1] != user:\n",
    "                node_list.append(int(diffusion_chain[i][1])+1)\n",
    "            else:\n",
    "                break\n",
    "        node_list = node_list[:max_nodes_length]\n",
    "        node_list += [0]*(max_nodes_length-len(node_list))\n",
    "    else:\n",
    "        node_list = [int(i[1])+1 for i in diffusion_chain][:max_nodes_length]\n",
    "        node_list += [0]*(max_nodes_length-len(node_list))\n",
    "    return node_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adoptByHour(diffusion_chain):\n",
    "    cnt_list = []\n",
    "    i = 0\n",
    "    while i < len(diffusion_chain):\n",
    "        if int(diffusion_chain[-1][0]) - int(diffusion_chain[0][0]) <= 3600: # one hour\n",
    "            cnt_list.append(len(diffusion_chain))\n",
    "            break\n",
    "        elif int(diffusion_chain[i][0]) - int(diffusion_chain[0][0]) <= 3600:\n",
    "            i += 1\n",
    "            continue\n",
    "        else:\n",
    "            cnt_list.append(i)\n",
    "            diffusion_chain = diffusion_chain[i:]\n",
    "            i = 0\n",
    "    return cnt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAdoption(user,mid,mid_history,max_adopt_length=120,pos=True):\n",
    "    diffusion_chain = mid_history[str(mid)]\n",
    "    if pos:\n",
    "        time_list, user_list = zip(*diffusion_chain)\n",
    "        index = user_list.index(user)\n",
    "        sub_diffusion = diffusion_chain[:index]\n",
    "#         print(sub_diffusion)\n",
    "        cnt_list = adoptByHour(sub_diffusion)\n",
    "#         print(cnt_list)\n",
    "        adopt_list = list(np.log(np.log(cnt_list)+1)[:max_adopt_length])\n",
    "#         print(adopt_list)\n",
    "        adopt_list += [0]*(max_adopt_length-len(adopt_list))\n",
    "#         print(adopt_list)\n",
    "    else:\n",
    "        cnt_list = adoptByHour(diffusion_chain)\n",
    "        rand = random.randint(0,len(cnt_list))\n",
    "        cnt_list = cnt_list[:rand]\n",
    "#         print(cnt_list)\n",
    "#         print(len(cnt_list))\n",
    "        adopt_list = list(np.log(np.log(cnt_list)+1)[:max_adopt_length])\n",
    "        adopt_list += [0]*(max_adopt_length-len(adopt_list))\n",
    "    return adopt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newsample(nnn, ratio):\n",
    "    if ratio > len(nnn):\n",
    "        return random.sample(nnn * (ratio // len(nnn) + 1), ratio)\n",
    "    else:\n",
    "        return random.sample(nnn, ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setWindowSize(history_list,period=6, threshold2=3):\n",
    "    timeLag = period * 30 * 24 * 60 *60 # count the seconds, period unit=month\n",
    "    l_start = []\n",
    "    l_end = []\n",
    "    sub_history = []\n",
    "    i = 0\n",
    "    while i < len(history_list):\n",
    "        if int(history_list[-1][1])-int(history_list[0][1]) < timeLag:\n",
    "            start = int(history_list[0][1])\n",
    "            end = int(history_list[-1][1])\n",
    "            l_start.append(start)\n",
    "            l_end.append(end)\n",
    "            sub_history.append(history_list)\n",
    "            break\n",
    "        elif int(history_list[i][1])-int(history_list[0][1]) < timeLag: # set it around 6 months\n",
    "            i += 1\n",
    "            continue\n",
    "        else:\n",
    "            start = int(history_list[0][1])\n",
    "            end = int(history_list[i][1])\n",
    "            l_start.append(start)\n",
    "            l_end.append(end)\n",
    "            sub_history.append(history_list[:i+1])\n",
    "            if len(history_list[i+1:])<threshold2:\n",
    "                break\n",
    "            else:\n",
    "                history_list = history_list[i+1:]\n",
    "                i = 0\n",
    "    return list(zip(l_start,l_end)), sub_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buid_train_data_T(user,mid_history,poslist,neglist,news_index,nb_history=20, npratio=4, max_nodes_length=30,max_adopt_length=120):\n",
    "    # pos,neg sample with mid\n",
    "    for pos_sample in poslist:\n",
    "        pos_neg_sample = newsample(neglist,npratio)\n",
    "        pos_neg_sample_node = []\n",
    "#         -----------------------\n",
    "        pos_neg_sample_adopt = []\n",
    "#         ----------------------------\n",
    "        for neg_sample in pos_neg_sample:\n",
    "            # negative nodes sampling\n",
    "            neg_node_list = getNodes(user=user, mid=neg_sample, mid_history=mid_history,pos=False)\n",
    "            pos_neg_sample_node.append(neg_node_list)\n",
    "#         -----------------------\n",
    "            neg_adopt_list = getAdoption(user=user,mid=neg_sample,mid_history=mid_history,pos=False)\n",
    "            pos_neg_sample_adopt.append(neg_adopt_list)\n",
    "#         ----------------------------\n",
    "        pos_neg_sample.append(pos_sample)\n",
    "        pos_node_list = getNodes(user=user, mid=pos_sample, mid_history=mid_history,pos=True)\n",
    "        pos_neg_sample_node.append(pos_node_list)\n",
    "        \n",
    "#         print(pos_sample)\n",
    "#         -------------------------\n",
    "        pos_adopt_list = getAdoption(user=user,mid=pos_sample,mid_history=mid_history,pos=True)\n",
    "        pos_neg_sample_adopt.append(pos_adopt_list)\n",
    "#         -------------------------\n",
    "\n",
    "        temp_label = [0] * npratio + [1]\n",
    "        temp_id = list(range(npratio + 1))\n",
    "        random.shuffle(temp_id)\n",
    "\n",
    "        shuffle_sample = []\n",
    "        shuffle_sample_node = []\n",
    "#         ----------------------\n",
    "        shuffle_sample_adopt = []\n",
    "#         ----------------------\n",
    "        shuffle_label = []\n",
    "\n",
    "        for id in temp_id:\n",
    "            shuffle_sample.append(int(pos_neg_sample[id]))\n",
    "            shuffle_sample_node.append(pos_neg_sample_node[id])\n",
    "#         ----------------------\n",
    "            shuffle_sample_adopt.append(pos_neg_sample_adopt[id])\n",
    "#         ----------------------\n",
    "            shuffle_label.append(temp_label[id])\n",
    "            \n",
    "        shuffle_sample = [news_index[int(i)] for i in shuffle_sample] # map to news index\n",
    "\n",
    "        posset = list(set(poslist)-set([pos_sample]))\n",
    "        allpos = [int(p) for p in random.sample(posset, min(nb_history, len(posset)))[:nb_history]]\n",
    "\n",
    "        allpos_node = []\n",
    "#         ----------------------\n",
    "        allpos_adopt = []\n",
    "#         ----------------------\n",
    "        for pos in allpos:\n",
    "            pos_node_list = getNodes(user=user, mid=pos, mid_history=mid_history,pos=True)\n",
    "            allpos_node.append(pos_node_list)\n",
    "#         ----------------------\n",
    "            pos_adopt_list = getAdoption(user=user, mid=pos, mid_history=mid_history,pos=True)\n",
    "            allpos_adopt.append(pos_adopt_list)\n",
    "#         ----------------------\n",
    "        allpos_node = allpos_node[:nb_history]\n",
    "        allpos_adopt = allpos_adopt[:nb_history] #####\n",
    "        allpos += [0] * (nb_history - len(allpos))\n",
    "        allpos_node += [[0]*max_nodes_length]*(nb_history-len(allpos_node))\n",
    "        allpos_adopt += [[0]*max_adopt_length]*(nb_history-len(allpos_adopt)) #####\n",
    "        \n",
    "        allpos = [news_index[int(i)] for i in allpos] # mapping to news index\n",
    "    \n",
    "    return shuffle_sample,shuffle_sample_node,shuffle_sample_adopt,shuffle_label,allpos,allpos_node,allpos_adopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_id = []\n",
    "all_train_pn = []\n",
    "all_train_nodes = []\n",
    "all_train_adopt = [] #####\n",
    "all_label = []\n",
    "\n",
    "all_test_id = []\n",
    "all_test_pn = []\n",
    "all_test_node = []\n",
    "all_test_adopt = [] #####\n",
    "all_test_label = []\n",
    "all_test_index = []\n",
    "\n",
    "all_user_pos = []\n",
    "all_user_pos_nodes = [] ## node in history\n",
    "all_user_pos_adopt = [] #####\n",
    "all_test_user_pos = []\n",
    "all_test_user_pos_nodes = []\n",
    "all_test_user_pos_adopt = [] #####\n",
    "npratio = 4\n",
    "nb_history = 20\n",
    "max_nodes_length = 30\n",
    "max_adopt_length = 120 #####\n",
    "nb_neg_sample = 10\n",
    "\n",
    "for user in uid_train:\n",
    "    history = user_history[user]\n",
    "    time_list,sub_his_list = setWindowSize(history,period=3,threshold2=3) # split user history under windowsize\n",
    "    for i in range(len(time_list)):\n",
    "#         print(len(time_list))\n",
    "        sub_mid = dict([k,v] for k,v in mid_history.items() if int(v[0][0])<=\\\n",
    "                       time_list[i][1] and int(v[-1][0])>=time_list[i][0])\n",
    "        sub_poslist = [h[0] for h in sub_his_list[i]]\n",
    "        sub_neglist = list(set(sub_mid.keys())-set(sub_poslist))\n",
    "        \n",
    "        shuffle_sample,shuffle_sample_node,shuffle_sample_adopt,shuffle_label,allpos,allpos_node,allpos_adopt = \\\n",
    "        buid_train_data_T(user,sub_mid,sub_poslist,sub_neglist,news_index)\n",
    "    \n",
    "        all_train_pn.append(shuffle_sample)\n",
    "        all_train_nodes.append(shuffle_sample_node)\n",
    "        all_train_adopt.append(shuffle_sample_adopt) #####\n",
    "        all_label.append(shuffle_label)\n",
    "        all_train_id.append(user)\n",
    "        all_user_pos.append(allpos)\n",
    "        all_user_pos_nodes.append(allpos_node)\n",
    "        all_user_pos_adopt.append(allpos_adopt) #####\n",
    "\n",
    "for user in uid_test:\n",
    "    test_history = user_history[user]\n",
    "    test_time_list,test_sub_his_list = setWindowSize(test_history,period=3,threshold2=3)\n",
    "    for i in range(len(test_time_list)):\n",
    "        test_sub_mid = dict([k,v] for k,v in mid_history.items() if int(v[0][0])<=\\\n",
    "                            test_time_list[i][1] and int(v[-1][0])>=test_time_list[i][0])\n",
    "        test_sub_poslist = [h[0] for h in test_sub_his_list[i]]\n",
    "        test_sub_neglist = list(set(test_sub_mid.keys())-set(test_sub_poslist))\n",
    "        \n",
    "        sess_index = []\n",
    "        sess_index.append(len(all_test_pn))\n",
    "        \n",
    "        for pos_sample in test_sub_poslist:\n",
    "            test_pos_node = getNodes(user=user, mid=pos_sample, mid_history=test_sub_mid,pos=True)\n",
    "            # ------------------\n",
    "            test_pos_adopt = getAdoption(user=user, mid=pos_sample, mid_history=test_sub_mid,pos=True)\n",
    "            # -------------------\n",
    "            test_posset = list(set(test_sub_poslist)-set([pos_sample]))\n",
    "            test_allpos = [int(p) for p in random.sample(test_posset, min(nb_history, len(test_posset)))[:nb_history]]\n",
    "            \n",
    "            test_allpos_node = []\n",
    "            # ------------------\n",
    "            test_allpos_adopt = []\n",
    "            # ------------------\n",
    "            for pos in test_allpos:\n",
    "                test_pos_node = getNodes(user=user, mid=pos, mid_history=test_sub_mid, pos=True)\n",
    "                test_allpos_node.append(test_pos_node)\n",
    "                # ------------------\n",
    "                test_pos_adopt = getAdoption(user=user, mid=pos, mid_history=test_sub_mid, pos=True)\n",
    "                test_allpos_adopt.append(test_pos_adopt)\n",
    "                # -------------------\n",
    "            test_allpos_node = test_allpos_node[:nb_history]\n",
    "            test_allpos_adopt = test_allpos_adopt[:nb_history] #####\n",
    "            test_allpos += [0]*(nb_history - len(test_allpos))\n",
    "            test_allpos_node += [[0]*max_nodes_length]*(nb_history-len(test_allpos_node))\n",
    "            test_allpos_adopt += [[0]*max_adopt_length]*(nb_history-len(test_allpos_adopt)) #####\n",
    "            \n",
    "            test_allpos = [news_index[int(i)] for i in test_allpos] # mapping\n",
    "            \n",
    "            all_test_pn.append(news_index[int(pos_sample)]) # mapping\n",
    "            all_test_node.append(test_pos_node)\n",
    "            all_test_adopt.append(test_pos_adopt) #####\n",
    "            all_test_label.append(1)\n",
    "            all_test_id.append(user)\n",
    "            all_test_user_pos.append(test_allpos)\n",
    "            all_test_user_pos_nodes.append(test_allpos_node)\n",
    "            all_test_user_pos_adopt.append(test_allpos_adopt) #####\n",
    "            \n",
    "        assert len(test_sub_neglist)>=nb_neg_sample\n",
    "        test_sub_neglist = random.sample(test_sub_neglist,nb_neg_sample)\n",
    "        \n",
    "        for neg_sample in test_sub_neglist:\n",
    "            test_neg_node = getNodes(user=user, mid=neg_sample, mid_history=test_sub_mid,pos=False)\n",
    "            # ----------\n",
    "            test_neg_adopt = getAdoption(user=user, mid=neg_sample, mid_history=test_sub_mid,pos=False)\n",
    "            # ----------\n",
    "            test_allpos_neg = [int(p) for p in random.sample(test_sub_poslist, min(nb_history, len(test_sub_poslist)))[:nb_history]]\n",
    "            \n",
    "            test_allpos_neg_node = []\n",
    "            # -----------------------------\n",
    "            test_allpos_neg_adopt = []\n",
    "            # -----------------------------\n",
    "            for pos in test_allpos_neg:\n",
    "                test_pos_node = getNodes(user=user, mid=pos, mid_history=test_sub_mid, pos=True)\n",
    "                test_allpos_neg_node.append(test_pos_node)\n",
    "                # ---------------------------\n",
    "                test_pos_adopt = getAdoption(user=user, mid=pos, mid_history=test_sub_mid, pos=True)\n",
    "                test_allpos_neg_adopt.append(test_pos_adopt)\n",
    "                # -----------------------------\n",
    "            test_allpos_neg_node = test_allpos_neg_node[:nb_history]\n",
    "            test_allpos_neg_adopt = test_allpos_neg_adopt[:nb_history] #####\n",
    "            test_allpos_neg += [0]*(nb_history - len(test_allpos_neg))\n",
    "            test_allpos_neg_node += [[0]*max_nodes_length]*(nb_history-len(test_allpos_neg_node))\n",
    "            test_allpos_neg_adopt += [[0]*max_adopt_length]*(nb_history-len(test_allpos_neg_adopt)) #####\n",
    "            \n",
    "            test_allpos_neg = [news_index[int(i)] for i in test_allpos_neg] # mapping\n",
    "            \n",
    "            all_test_pn.append(news_index[int(neg_sample)]) # mapping\n",
    "            all_test_node.append(test_neg_node)\n",
    "            all_test_adopt.append(test_neg_adopt) #####\n",
    "            all_test_label.append(0)\n",
    "            all_test_id.append(user)\n",
    "            all_test_user_pos.append(test_allpos_neg)\n",
    "            all_test_user_pos_nodes.append(test_allpos_neg_node)\n",
    "            all_test_user_pos_adopt.append(test_allpos_neg_adopt) ########\n",
    "            \n",
    "        sess_index.append(len(all_test_pn))\n",
    "        all_test_index.append(sess_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_pn = np.array(all_train_pn, dtype='int32')\n",
    "all_train_nodes = np.array(all_train_nodes,dtype='int32')\n",
    "all_train_adopt = np.array(all_train_adopt,dtype='float32') ###\n",
    "all_label = np.array(all_label, dtype='int32')\n",
    "all_train_id = np.array(all_train_id, dtype='int32')\n",
    "all_user_pos = np.array(all_user_pos, dtype='int32')\n",
    "all_user_pos_nodes = np.array(all_user_pos_nodes,dtype='int32')\n",
    "all_user_pos_adopt = np.array(all_user_pos_adopt,dtype='float32') ###\n",
    "\n",
    "all_test_pn = np.array(all_test_pn, dtype='int32')\n",
    "all_test_node = np.array(all_test_node,dtype='int32')\n",
    "all_test_adopt = np.array(all_test_adopt,dtype='float32') ###\n",
    "all_test_label = np.array(all_test_label, dtype='int32')\n",
    "all_test_id = np.array(all_test_id, dtype='int32')\n",
    "all_test_user_pos = np.array(all_test_user_pos, dtype='int32')\n",
    "all_test_user_pos_nodes = np.array(all_test_user_pos_nodes,dtype='int32')\n",
    "all_test_user_pos_adopt = np.array(all_test_user_pos_adopt,dtype='float32') ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pkl('data_T_3m_nb10_120_test_1', news_words,word_embedding_matrix,node_embedding_matrix,\\\n",
    "         all_user_pos,all_user_pos_nodes,all_user_pos_adopt,all_train_pn,all_train_nodes,all_train_adopt,all_label,all_train_id,\\\n",
    "         all_test_user_pos,all_test_user_pos_nodes,all_test_user_pos_adopt,all_test_pn,all_test_node,all_test_adopt,all_test_label,all_test_id,\n",
    "         all_test_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (shap)",
   "language": "python",
   "name": "pycharm-81b7abe8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
